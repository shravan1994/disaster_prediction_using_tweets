{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DzphC5WThxtk"
      ],
      "mount_file_id": "1Wre6UQ02Nn5kx35748CvNshTwDtsCUCk",
      "authorship_tag": "ABX9TyPsXB7AcKNoKsrrXfRaExgb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shravan1994/disaster_prediction_using_tweets/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "H_gPEeUZi5mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "!pip install -q -U \"tensorflow-text==2.8.*\"\n",
        "!pip install bert-tensorflow\n",
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4nXmMp0-w5_",
        "outputId": "848bb7d0-d333-404e-c15b-e369a0ac8d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 14.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.0 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-tensorflow\n",
            "  Downloading bert_tensorflow-1.0.4-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.8.2)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.26.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (14.0.6)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.47.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (4.1.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.5.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, Conv1D, MaxPool1D, Flatten\n",
        "from tensorflow.keras import Model\n",
        "from bert.tokenization import FullTokenizer\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import urllib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.sparse import hstack\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "OClt8rmBURMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "QWz_SNQLUClb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4', trainable=True)"
      ],
      "metadata": {
        "id": "npofGw5LXtkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_architecture():\n",
        "  input_layer1 = Input(shape=(), dtype=tf.string, name='text-layer')\n",
        "  text_preprocessed = bert_preprocess_model(input_layer1)\n",
        "  bert_outputs = bert_layer(text_preprocessed)\n",
        "  pooled_out = bert_outputs['pooled_output']\n",
        "  dropout_1 = Dropout(0.1)(pooled_out)\n",
        "  bert_out = tf.keras.layers.Reshape((32,24))(dropout_1)\n",
        "      \n",
        "  lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(bert_out)\n",
        "  dropout_3 = tf.keras.layers.Dropout(0.1)(lstm)\n",
        "  cnn = tf.keras.layers.Conv1D(64,5)(dropout_3)\n",
        "\n",
        "  max_pool = tf.keras.layers.MaxPooling1D(25)(cnn)\n",
        "  flat = tf.keras.layers.Flatten(data_format='channels_last')(max_pool)\n",
        "\n",
        "  dense1 = tf.keras.layers.Dense(128,activation='relu')(pooled_out)\n",
        "  dense2 = tf.keras.layers.Dense(64,activation='relu')(dense1)\n",
        "  dropout_2 = tf.keras.layers.Dropout(0.1)(dense2)\n",
        "\n",
        "  concat = tf.keras.layers.Concatenate()([dropout_2,flat])\n",
        "\n",
        "  output = Dense(1,activation='sigmoid')(concat)\n",
        "\n",
        "  model_bert_cnn = Model(inputs=input_layer1, outputs=output)\n",
        "  return model_bert_cnn"
      ],
      "metadata": {
        "id": "KXZmB64EUCrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods for data preprocessing"
      ],
      "metadata": {
        "id": "DzphC5WThxtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def replace_byte_chars(tweet):\n",
        "  tweet = re.sub(r\"\\x89Ûªs\", '\\'s', tweet)\n",
        "  tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89ûó\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89Û\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "  tweet = re.sub(r\"åÊ\", \" \", tweet)\n",
        "  tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "  tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
        "  tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "  tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "  tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "  tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "  tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "  tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "  \n",
        "  return tweet"
      ],
      "metadata": {
        "id": "fAY6yqzltxfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "def do_decontractions(tweet):\n",
        "  tweet = contractions.fix(tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "9cqEdolF0vWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet):\n",
        "  tweet = tweet.lower()\n",
        "  tweet = replace_byte_chars(tweet)\n",
        "  tweet = do_decontractions(tweet)\n",
        "  # removing urls\n",
        "  tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "  # Words with punctuations and special characters\n",
        "  tweet = re.sub(r\"[\\\"#$%&'()*+,\\-.\\/:;<=>@[\\]^_`{|}~]\", \"\", tweet)\n",
        "  # adding space in front of ? and !\n",
        "  tweet = re.sub(r\"([?!]+)\", r\" \\1\", tweet)\n",
        "  tweet = re.sub(r\"\\s+\", \" \", tweet)\n",
        "  tweet = tweet.strip()\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "-0Y8l9wP1RmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## final_func_1(), this method returns predicted target labels"
      ],
      "metadata": {
        "id": "Nlrv3fuHbc9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXaMju4a1xR"
      },
      "outputs": [],
      "source": [
        "def final_fun_1(X):\n",
        "  model_path = '/content/drive/MyDrive/Colab Notebooks/Self case study/Natutal Disaster prediction/model/best_bert_cnn_model.h5'\n",
        "\n",
        "  if type(X) == str:\n",
        "    X = pd.Series([X]) \n",
        "  \n",
        "  X = X.apply(lambda x: clean_tweet(x))\n",
        "  \n",
        "  model = get_model_architecture()\n",
        "  model.load_weights(model_path)\n",
        "\n",
        "  target_predicted = model.predict(X)\n",
        "  target_predicted = [1 if target > 0.5 else 0 for target in target_predicted]\n",
        "\n",
        "  return target_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## final_func_2(), this method returns log loss"
      ],
      "metadata": {
        "id": "NEveeoNrbjFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_fun_2(X, y):\n",
        "  target_predicted = final_fun_1(X)\n",
        "  f1 = f1_score(y, target_predicted, average='macro')\n",
        "  return f1"
      ],
      "metadata": {
        "id": "XAqat7gMbjO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing above two functions"
      ],
      "metadata": {
        "id": "jsKSc-pwcFeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \".\"\n",
        "\n",
        "!kaggle competitions download -c nlp-getting-started\n",
        "!unzip nlp-getting-started.zip -d 'data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByxiqPM-mpSV",
        "outputId": "57153300-0e61-4825-e0f8-09aae78c88d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n",
            "Downloading nlp-getting-started.zip to /content\n",
            "  0% 0.00/593k [00:00<?, ?B/s]\n",
            "100% 593k/593k [00:00<00:00, 102MB/s]\n",
            "Archive:  nlp-getting-started.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "  inflating: data/test.csv           \n",
            "  inflating: data/train.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing func 1"
      ],
      "metadata": {
        "id": "oPFIBQIOmwLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_df = pd.read_csv('data/test.csv')\n",
        "predicted_targets = final_fun_1(X_df['text'])\n",
        "predicted_targets[0:20]"
      ],
      "metadata": {
        "id": "HaUsUegtmwLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac54781f-12fb-4a96-d297-a56915676f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing func 2"
      ],
      "metadata": {
        "id": "hh00RqjvmgXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_df = pd.read_csv('data/train.csv')\n",
        "f1 = final_fun_2(X_df['text'], X_df['target'])\n",
        "print('F1 score: ', f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVoyByi7hBPv",
        "outputId": "be12bb4b-7417-49ca-ae66-e300a06091b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score:  0.8500380099963321\n"
          ]
        }
      ]
    }
  ]
}